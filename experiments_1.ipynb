{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.retiro_model import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Attention Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetiroAttention(nn.Module):\n",
    "    def __init__(self, chan, key_dim, value_dim, heads, kernel_size,padding,stride):\n",
    "        super(RetiroAttention, self).__init__()\n",
    "        self.query  = nn.Conv2d(chan, key_dim * heads, kernel_size, padding=padding, stride=stride)\n",
    "        self.key    = nn.Conv2d(chan, key_dim * heads, kernel_size, padding=padding, stride=stride)\n",
    "        self.value  = nn.Conv2d(chan, value_dim * heads, kernel_size, padding=padding, stride=stride)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(0.0)\n",
    "        self.proj = nn.Conv2d(value_dim * heads, chan, kernel_size, padding=padding)\n",
    "        self.n_head = heads\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q = self.query(x.permute(0,3,1,2)).flatten(start_dim=-2).transpose(1, 2)\n",
    "        k = self.key(x.permute(0,3,1,2)).flatten(start_dim=-2).transpose(1, 2)\n",
    "        v = self.value(x.permute(0,3,1,2)).flatten(start_dim=-2).transpose(1, 2)\n",
    "\n",
    "        print(q.shape)\n",
    "        B, C, T = q.size()\n",
    "        print( B, C, T)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        print(f'Linear Attention: q {q.shape} k {k.shape} v {v.shape}')\n",
    "\n",
    "        k = k.softmax(dim=-1)   #\n",
    "        k_cumsum = k.sum(dim=[-2], keepdim=True)\n",
    "        D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        \n",
    "        context = k.transpose(-2, -1) @ v\n",
    "        y = self.attn_drop((q @ context) * D_inv + q)\n",
    "        y1 = rearrange(y, 'b h n d -> b n (h d)')\n",
    "        y2 =torch.einsum('bhnd->bnhd',y)\n",
    "        print(f'y: {y.shape}, y1: {y1.shape}, y2: {y2.shape}')\n",
    "        out = self.proj(y.reshape(B, -1, T, T))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, attn_pdrop=0.0):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.attn_type = 'l1'\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B, T1, C = x.size()\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q = self.query(x)#.view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        k = self.key(x)#.view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = self.value(x)#.view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        print(f'Linear Attention: q {q.shape} k {k.shape} v {v.shape}')\n",
    "\n",
    "        print(f'Linear Attention: q {q.view(B, T1, self.n_head, C // self.n_head).shape}')\n",
    "        print(f'Linear Attention: q {q.view(B, T1, self.n_head, C // self.n_head).transpose(1, 2).shape}')\n",
    "\n",
    "        q = q.view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        k = k.softmax(dim=-1)   #\n",
    "        print(k.shape)\n",
    "        k_cumsum = k.sum(dim=-2, keepdim=True)\n",
    "        print(k_cumsum.shape)\n",
    "        D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)   \n",
    "        print(D_inv.shape)\n",
    "\n",
    "        print(f'k {k.shape}')\n",
    "        print(f'k {k.transpose(-2, -1).shape}')\n",
    "        context = k.transpose(-2, -1) @ v\n",
    "        print(context.shape)\n",
    "        y = self.attn_drop((q @ context) * D_inv + q)\n",
    "        print(y.shape)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Attention: q torch.Size([4, 128, 64]) k torch.Size([4, 128, 64]) v torch.Size([4, 128, 64])\n",
      "Linear Attention: q torch.Size([4, 128, 1, 64])\n",
      "Linear Attention: q torch.Size([4, 1, 128, 64])\n",
      "torch.Size([4, 1, 128, 64])\n",
      "torch.Size([4, 1, 1, 64])\n",
      "torch.Size([4, 1, 128, 1])\n",
      "k torch.Size([4, 1, 128, 64])\n",
      "k torch.Size([4, 1, 64, 128])\n",
      "torch.Size([4, 1, 64, 64])\n",
      "torch.Size([4, 1, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand([4,128,64])\n",
    "layer1 = LinearAttention(n_embd=64, n_head=1)\n",
    "output = layer1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4096, 64])\n",
      "4 4096 64\n",
      "Linear Attention: q torch.Size([4, 2, 64, 2048]) k torch.Size([4, 2, 64, 2048]) v torch.Size([4, 2, 64, 2048])\n",
      "y: torch.Size([4, 2, 64, 2048]), y1: torch.Size([4, 64, 4096]), y2: torch.Size([4, 64, 2, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 63, 63])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand([4,128,128,32])\n",
    "layer2 = RetiroAttention(chan=32, key_dim=32, value_dim=32, heads=2, kernel_size=2,padding=0,stride=2)\n",
    "output = layer2(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-961ed67232b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
